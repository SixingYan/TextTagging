{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\10/'    \n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "version='1.0.0'\n",
    "torch_url=f\"http://download.pytorch.org/whl/{accelerator}/torch-{version}-{platform}-linux_x86_64.whl\"\n",
    "\n",
    "!pip install -U {torch_url} torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls drive/Colab/datagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "local = True\n",
    "if local:\n",
    "    DATAPATH = '/Users/alfonso/workplace/datagrad/Data'\n",
    "    MODELPATH = '/Users/alfonso/workplace/datagrad/Model'\n",
    "    \n",
    "else:\n",
    "    PATH = '/content/drive/'\n",
    "    DATAPATH = os.path.join(PATH,'Colab/datagrad/data/')\n",
    "    MODELPATH = os.path.join(PATH,'Colab/datagrad/model/')\n",
    "\n",
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "tag_to_ix = {\"a\": 0, \"b\": 1, \"c\": 2, \"o\": 3,\n",
    "             START_TAG: 4, STOP_TAG: 5}\n",
    "ix_to_tag = {tp[1]: tp[0] for tp in tag_to_ix.items()}\n",
    "ix_to_tag[5] = \"o\"\n",
    "ix_to_tag[4] = \"o\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromPickle(path):\n",
    "    var = None\n",
    "    with open(path, 'rb') as f:\n",
    "        var = pickle.load(f)\n",
    "    return var\n",
    "\n",
    "def load():\n",
    "    trn_X = fromPickle(os.path.join(\n",
    "        DATAPATH, 'trn_X_token.pickle'))[:10]\n",
    "    trn_y = fromPickle(os.path.join(\n",
    "        DATAPATH, 'trn_y_token.pickle'))[:10]\n",
    "    tst = fromPickle(os.path.join(\n",
    "        DATAPATH, 'tst_X_token.pickle'))[:10]\n",
    "    word_to_ix = fromPickle(os.path.join(\n",
    "        DATAPATH, 'word_to_ix.pickle'))\n",
    "    return trn_X, trn_y, tst, word_to_ix\n",
    "\n",
    "def savemodel(model, name):\n",
    "    torch.save(model.state_dict(), os.path.join(MODELPATH, name))\n",
    "\n",
    "def evalinfo(y_preds, y_trues):\n",
    "    size = len(y_preds)\n",
    "    assert len(y_preds) == len(y_trues)\n",
    "    stat = {'a': [0, 0], 'b': [0, 0], 'c': [0, 0]}\n",
    "    tags = ['a', 'b', 'c']\n",
    "    prec = {'a': [], 'b': [], 'c': []}\n",
    "    recl = {'a': [], 'b': [], 'c': []}\n",
    "    f1 = {'a': 0, 'b': 0, 'c': 0}\n",
    "    for j in range(len(y_preds)):\n",
    "        y_pred, y_true = y_preds[j], y_trues[j]\n",
    "        prestat, recstat = copy.copy(stat), copy.copy(stat)\n",
    "        for i in range(len(y_pred)):\n",
    "            t = ix_to_tag[int(y_true[i])]\n",
    "            p = ix_to_tag[int(y_pred[i])]\n",
    "            if p in tags:\n",
    "                prestat[p][1] += 1\n",
    "            if p in tags and p == t:\n",
    "                prestat[p][0] += 1\n",
    "                recstat[t][0] += 1\n",
    "            if t in tags:\n",
    "                recstat[t][1] += 1\n",
    "        for x in tags:\n",
    "            if recstat[x][1] != 0:\n",
    "                recl[x].append(recstat[x][0] /recstat[x][1])\n",
    "            if prestat[x][1] != 0:\n",
    "                prec[x].append(prestat[x][0] / prestat[x][1])\n",
    "    for x in tags:\n",
    "        prec[x] = 0 if len(prec[x]) == 0 else sum(prec[x])/len(prec[x])\n",
    "        recl[x] = 0 if len(recl[x]) == 0 else sum(recl[x])/len(recl[x])\n",
    "        f1[x] = (2 * prec[x] * recl[x]) / (prec[x] + recl[x] + 1e-8)\n",
    "\n",
    "    for x in tags:\n",
    "        print('TAG {} \\t prec {:.4f} \\t recl {:.4f} \\t f1 {:.4f}'.format(\n",
    "            x, prec[x], recl[x], f1[x]))\n",
    "\n",
    "    print('AVG prec {:.4f} \\t recl {:.4f} \\t f1 {:.4f}'.format(\n",
    "        sum(prec[x] for x in tags) / 3,\n",
    "        sum(recl[x] for x in tags) / 3,\n",
    "        sum(f1[x] for x in tags) / 3))\n",
    "    \n",
    "def argmax(vec):\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + \\\n",
    "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "def init_embedding(embedding):\n",
    "    bias = np.sqrt(3.0 / embedding.embedding_dim)\n",
    "    nn.init.uniform_(embedding.weight, -bias, bias)\n",
    "\n",
    "def init_linear(linear):\n",
    "    bias = np.sqrt(6.0 / (linear.weight.size(0) +\n",
    "                          linear.weight.size(1)))\n",
    "    nn.init.uniform_(linear.weight, -bias, bias)\n",
    "    if linear.bias is not None:\n",
    "        linear.bias.data.zero_()\n",
    "\n",
    "def init_rnn(input_rnn, rnn='lstm'):\n",
    "    part_num = 4 if rnn == 'lstm' else 3\n",
    "    for ind in range(0, input_rnn.num_layers):\n",
    "        weight = eval('input_rnn.weight_ih_l' + str(ind))\n",
    "        hid_size = weight.size(0) // part_num\n",
    "        for i in range(part_num):\n",
    "            nn.init.xavier_normal_(weight[hid_size * i:hid_size * (i + 1), :])\n",
    "        weight = eval('input_rnn.weight_hh_l' + str(ind))\n",
    "        for i in range(part_num):\n",
    "            nn.init.xavier_normal_(weight[hid_size * i:hid_size * (i + 1), :])\n",
    "    if input_rnn.bidirectional:\n",
    "        for ind in range(0, input_rnn.num_layers):\n",
    "            weight = eval('input_rnn.weight_ih_l' + str(ind) + '_reverse')\n",
    "            for i in range(part_num):\n",
    "                nn.init.xavier_normal_(\n",
    "                    weight[hid_size * i:hid_size * (i + 1), :])\n",
    "            weight = eval('input_rnn.weight_hh_l' + str(ind) + '_reverse')\n",
    "            for i in range(part_num):\n",
    "                nn.init.xavier_normal_(\n",
    "                    weight[hid_size * i:hid_size * (i + 1), :])\n",
    "    if input_rnn.bias:\n",
    "        for ind in range(0, input_rnn.num_layers):\n",
    "            bias = eval('input_rnn.bias_ih_l' + str(ind))\n",
    "            bias.data.zero_()\n",
    "            bias.data[input_rnn.hidden_size: 2 * input_rnn.hidden_size] = 1\n",
    "            bias = eval('input_rnn.bias_hh_l' + str(ind))\n",
    "            bias.data.zero_()\n",
    "            bias.data[input_rnn.hidden_size: 2 * input_rnn.hidden_size] = 1\n",
    "        if input_rnn.bidirectional:\n",
    "            for ind in range(0, input_rnn.num_layers):\n",
    "                bias = eval('input_rnn.bias_ih_l' + str(ind) + '_reverse')\n",
    "                bias.data.zero_()\n",
    "                bias.data[input_rnn.hidden_size: 2 *\n",
    "                          input_rnn.hidden_size] = 1\n",
    "                bias = eval('input_rnn.bias_hh_l' + str(ind) + '_reverse')\n",
    "                bias.data.zero_()\n",
    "                bias.data[input_rnn.hidden_size: 2 *\n",
    "                          input_rnn.hidden_size] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim=10, hid_dim=10, loss_fn=nn.NLLLoss()):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.LSTM(embed_dim, hid_dim, bidirectional=True)\n",
    "        self.hid2tag = nn.Linear(hid_dim * 2, len(tag_to_ix))\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def _forward(self, x):\n",
    "        embeds = self.word_embeds(x)\n",
    "        lstm_out, _ = self.rnn(embeds.view(len(x), 1, -1))\n",
    "        tag_space = self.hid2tag(lstm_out.view(len(x), -1))\n",
    "        output = F.log_softmax(tag_space, dim=1)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._forward(x)\n",
    "        tag_seq = torch.argmax(output, dim=1).numpy()\n",
    "        return tag_seq\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        output = self._forward(x)\n",
    "        return self.loss_fn(output, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiLSTM + CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMCRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim=10, hid_dim=20, layer_num=2, device='cpu'):\n",
    "        super(BiLSTMCRF, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tags_size = len(tag_to_ix)\n",
    "        self.layer_num = layer_num\n",
    "        self.device = device\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.LSTM(embed_dim, hid_dim // 2,\n",
    "                           num_layers=self.layer_num, bidirectional=True, dropout=0.1)\n",
    "        init_rnn(self.rnn, 'lstm')\n",
    "        self.hid2tag = nn.Linear(hid_dim, self.tags_size)\n",
    "        init_linear(self.hid2tag)\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tags_size, self.tags_size))\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (nn.init.xavier_uniform_(torch.empty(2 * self.layer_num, 1, self.hid_dim // 2), gain=nn.init.calculate_gain('relu')),\n",
    "                nn.init.xavier_uniform_(torch.empty(2 * self.layer_num, 1, self.hid_dim // 2), gain=nn.init.calculate_gain('relu')))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        init_alphas = torch.full((1, self.tags_size), -10000.)\n",
    "        init_alphas[0][tag_to_ix[START_TAG]] = 0.\n",
    "        forward_var = init_alphas\n",
    "        for feat in feats:\n",
    "            alphas_t = []\n",
    "            for next_tag in range(self.tags_size):\n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.tags_size)\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, x):\n",
    "        hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(x).view(\n",
    "            len(x), 1, -1)\n",
    "        lstm_out, _ = self.rnn(embeds, hidden)\n",
    "        lstm_out = lstm_out.view(len(x), self.hid_dim)\n",
    "        lstm_feats = self.hid2tag(lstm_out)\n",
    "\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat(\n",
    "            [torch.tensor([tag_to_ix[START_TAG]], dtype=torch.long, device=self.device), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + \\\n",
    "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "        init_vvars = torch.full((1, self.tags_size), -10000.)\n",
    "        init_vvars[0][tag_to_ix[START_TAG]] = 0\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []\n",
    "            viterbivars_t = []\n",
    "            for next_tag in range(self.tags_size):\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "        terminal_var = forward_var + \\\n",
    "            self.transitions[tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        start = best_path.pop()\n",
    "        assert start == tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()  # 把从后向前的路径正过来\n",
    "        return best_path\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        feats = self._get_lstm_features(x)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, y)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, x):  # dont confuse this with _forward_alg above.\n",
    "        lstm_feats = self._get_lstm_features(x)\n",
    "        tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return tag_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder + Att + Decoder + CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderAttDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, in_hdim=128, out_hdim=128, de_hdim: int=128, loss_fn=nn.NLLLoss(),\n",
    "                 use_crf=True, max_length=1000,\n",
    "                 bi=2, device='cpu', dropout=0.4, num_layers=2,\n",
    "                 teacher_forcing_ratio=0.3):\n",
    "        super(EncoderAttDecoder, self).__init__()\n",
    "        self.bi = bi\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        self.loss_fn = loss_fn if use_crf is False else None\n",
    "        self.num_layers = num_layers\n",
    "        self.en_in_hdim = in_hdim\n",
    "        self.en_out_hdim = out_hdim\n",
    "        self.tags_size = len(tag_to_ix)\n",
    "        self.de_hdim = de_hdim\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.use_crf = use_crf\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.en_wrd_embed = nn.Embedding(vocab_size, self.en_in_hdim)\n",
    "        init_embedding(self.en_wrd_embed)\n",
    "        self.enrnn = nn.GRU(self.en_in_hdim, self.en_out_hdim, num_layers=num_layers,\n",
    "                            bidirectional=True if bi == 2 else False,\n",
    "                            dropout=dropout)\n",
    "        init_rnn(self.enrnn, 'GRU')\n",
    "        self.de_embed = nn.Embedding(self.tags_size, self.de_hdim)\n",
    "        init_embedding(self.de_embed)\n",
    "        self.attn = nn.Linear(self.de_hdim * 2, self.max_length)\n",
    "        init_linear(self.attn)\n",
    "        self.attn_combine = nn.Linear(self.de_hdim * 2, self.de_hdim)\n",
    "        init_linear(self.attn_combine)\n",
    "        self.dernn = nn.GRU(self.de_hdim, self.de_hdim)  # , dropout=0.3)\n",
    "        init_rnn(self.dernn, 'GRU')\n",
    "        self.hid2tag = nn.Linear(self.de_hdim, self.tags_size)\n",
    "        init_linear(self.hid2tag)\n",
    "        self.transitions = nn.Parameter(torch.nn.init.uniform_(\n",
    "            torch.empty(self.tags_size, self.tags_size)))\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "    def _encoder(self, x):\n",
    "        eoutputs = torch.zeros(\n",
    "            self.max_length, self.en_out_hdim * self.bi, device=self.device)\n",
    "        ehidden = nn.init.xavier_uniform_(\n",
    "            torch.zeros(self.bi * self.num_layers, 1,\n",
    "                        self.en_out_hdim, device=self.device),\n",
    "            gain=nn.init.calculate_gain('relu'))\n",
    "        for ei in range(x.size(0)):\n",
    "            eoutput, ehidden = self._encoder_net(\n",
    "                x[ei], ehidden)\n",
    "            eoutputs[ei] = eoutput[0, 0]\n",
    "\n",
    "        return eoutputs, ehidden\n",
    "\n",
    "    def _encoder_net(self, sentence, hidden):\n",
    "        embed = self.en_wrd_embed(sentence).view(1, 1, -1)\n",
    "        output, hidden = self.enrnn(self.dropout(embed), hidden)\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        return output, hidden\n",
    "\n",
    "    def _decoder(self, eoutputs, ehidden, y=None):\n",
    "        dinput = torch.tensor([[tag_to_ix[START_TAG]]], device=self.device)\n",
    "        dhidden = torch.cat(\n",
    "            [ehidden[0, :, :], ehidden[-1, :, :]], 1).unsqueeze(0)\n",
    "        doutputs = torch.zeros(\n",
    "            self.length, self.de_hdim, device=self.device)\n",
    "        if y is not None:\n",
    "            for di in range(self.length):\n",
    "                doutput, dhidden = self._decoder_net(\n",
    "                    dinput, dhidden, eoutputs)\n",
    "                dinput = y[di]  # Teacher forcing\n",
    "                doutputs[di] = doutput[0, 0]\n",
    "        else:\n",
    "            for di in range(self.length):\n",
    "                doutput, decoder_hidden = self._decoder_net(\n",
    "                    dinput, dhidden, eoutputs)\n",
    "                topv, topi = doutput.topk(1)\n",
    "                dinput = topi.squeeze().detach()\n",
    "                doutputs[di] = doutput[0, 0]\n",
    "                if dinput.item() == STOP_TAG:\n",
    "                    break\n",
    "        return doutputs\n",
    "\n",
    "    def _decoder_net(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.de_embed(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.dernn(output, hidden)\n",
    "        output = self.dropout(output)\n",
    "        output = F.log_softmax(self.hid2tag(output[0]), dim=1)\n",
    "        return output, hidden\n",
    "\n",
    "    def _crf(self, feats):\n",
    "        return self._viterbi_decode(feats)\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        init_alphas = torch.full((1, self.tags_size), -10000.)\n",
    "        init_alphas[0][tag_to_ix[START_TAG]] = 0.\n",
    "        forward_var = init_alphas\n",
    "        # Iterate through the sentence\n",
    "        for feat in feats:\n",
    "            alphas_t = []\n",
    "            for next_tag in range(self.tags_size):\n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.tags_size)\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat(\n",
    "            [torch.tensor([tag_to_ix[START_TAG]], dtype=torch.long, device=self.device), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score += self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score += self.transitions[tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _neg_log_likelihood(self, feats, tags):\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "        init_vvars = torch.full((1, self.tags_size), -10000.)\n",
    "        init_vvars[0][tag_to_ix[START_TAG]] = 0\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []\n",
    "            viterbivars_t = []\n",
    "            for next_tag in range(self.tags_size):\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "        terminal_var = forward_var + \\\n",
    "            self.transitions[tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        start = best_path.pop()\n",
    "        assert start == tag_to_ix[START_TAG]\n",
    "        best_path.reverse()\n",
    "        return best_path\n",
    "\n",
    "    def _get_feat(self, x, y=None, use_tf=False):\n",
    "        self.length = x.size(0)\n",
    "        eoutputs, ehidden = self._encoder(x)\n",
    "        if use_tf and random.random() < self.teacher_forcing_ratio:\n",
    "            doutputs = self._decoder(eoutputs, ehidden, y)\n",
    "        else:\n",
    "            doutputs = self._decoder(eoutputs, ehidden)\n",
    "        output = self.hid2tag(doutputs)\n",
    "        return output\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        output = self._get_feat(x, y, use_tf=True)\n",
    "        if self.use_crf:\n",
    "            return self._neg_log_likelihood(output, y)\n",
    "        else:\n",
    "            return self.loss_fn(output, y)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._get_feat(x)\n",
    "        if self.use_crf:\n",
    "            tag_seq = self._crf(output)\n",
    "        else:\n",
    "            tag_seq = torch.argmax(output, dim=1).numpy()\n",
    "        return tag_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_X, trn_y, tst, word_to_ix = load()\n",
    "max_length = max(len(x) for x in trn_X)\n",
    "def train(trn_ixs, model, optimizer):\n",
    "    avg_loss = 0\n",
    "    tsize = len(trn_ixs)\n",
    "    for i in tqdm(np.random.permutation(trn_ixs)):\n",
    "        x = torch.tensor(trn_X[i], dtype=torch.long, device=device)\n",
    "        y = torch.tensor(trn_y[i], dtype=torch.long, device=device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(x, y)\n",
    "        avg_loss += loss.item() / tsize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model, avg_loss\n",
    "def evaluate(vld_ixs, model):\n",
    "    vld_loss = 0\n",
    "    vsize = len(vld_ixs)\n",
    "    y_preds, y_trues = [], []\n",
    "    for i in tqdm(np.random.permutation(vld_ixs)):\n",
    "        x = torch.tensor(trn_X[i], dtype=torch.long, device=device)\n",
    "        y = torch.tensor(trn_y[i], dtype=torch.long, device=device)\n",
    "        y_pred = model(x)\n",
    "        loss = model.loss(x, y)\n",
    "        vld_loss += loss.item() / vsize\n",
    "        y_preds.append(y_pred[:])\n",
    "        y_trues.append(trn_y[i])\n",
    "    return vld_loss, y_preds, y_trues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_num = 5\n",
    "epoch_num = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "direction = 2\n",
    "en_indim = 64\n",
    "en_outdim = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [00:00<00:00, 33.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPLIT 1 --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 33.70it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 49.48it/s]\n",
      " 50%|█████     | 4/8 [00:00<00:00, 31.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 \t avg_loss 1.1474 \t vld_loss 0.5510 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 31.26it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 41.42it/s]\n",
      " 38%|███▊      | 3/8 [00:00<00:00, 26.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 \t avg_loss 0.4863 \t vld_loss 0.5955 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "SPLIT 2 --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 28.92it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 105.13it/s]\n",
      " 25%|██▌       | 2/8 [00:00<00:00, 19.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 \t avg_loss 1.0002 \t vld_loss 0.2338 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 29.60it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 102.17it/s]\n",
      " 38%|███▊      | 3/8 [00:00<00:00, 29.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 \t avg_loss 0.4069 \t vld_loss 0.3439 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "SPLIT 3 --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 30.46it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 96.70it/s]\n",
      " 38%|███▊      | 3/8 [00:00<00:00, 23.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 \t avg_loss 0.9061 \t vld_loss 1.2514 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 26.35it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 124.05it/s]\n",
      " 50%|█████     | 4/8 [00:00<00:00, 37.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 \t avg_loss 0.3463 \t vld_loss 0.9454 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "SPLIT 4 --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 35.68it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 94.09it/s]\n",
      " 38%|███▊      | 3/8 [00:00<00:00, 24.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 \t avg_loss 0.9722 \t vld_loss 0.9596 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 28.13it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 82.23it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 44.64it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 \t avg_loss 0.2737 \t vld_loss 0.8244 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "SPLIT 5 --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 33.68it/s]\n",
      " 62%|██████▎   | 5/8 [00:00<00:00, 36.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 \t avg_loss 1.0243 \t vld_loss 0.6825 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 38.28it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 35.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 \t avg_loss 0.3434 \t vld_loss 0.6073 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for sid in range(split_num):\n",
    "    trn_ixs, vld_ixs = train_test_split(\n",
    "        list(range(len(trn_X))), test_size=1 / split_num,\n",
    "        shuffle=True, random_state=sid)\n",
    "    print('SPLIT {} --------------------'.format(sid + 1))\n",
    "    model = BiLSTM(len(word_to_ix), embed_dim=64, hid_dim=64).to(device)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=0.01, weight_decay=1e-6)\n",
    "    for epoch in range(epoch_num):\n",
    "        model.train()\n",
    "        model, avg_loss = train(trn_ixs, model, optimizer)\n",
    "        model.eval()\n",
    "        vld_loss, y_preds, y_trues = evaluate(vld_ixs, model)\n",
    "        print('Epoch {}/{} \\t avg_loss {:.4f} \\t vld_loss {:.4f} \\t'.format(\n",
    "            epoch + 1, epoch_num, avg_loss, vld_loss))\n",
    "        evalinfo(y_preds, y_trues)\n",
    "        savemodel(model, 'bilstm_{}_{}.pytorch'.format(sid,epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiLSTM + CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [00:00<00:00,  9.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPLIT 1 --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 11.16it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 11.94it/s]\n",
      " 25%|██▌       | 2/8 [00:00<00:00, 10.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 \t avg_loss 26.5289 \t vld_loss 48.6112 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 11.63it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 12.54it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 \t avg_loss 13.6288 \t vld_loss 51.6336 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "SPLIT 2 --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00,  8.76it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 34.29it/s]\n",
      " 25%|██▌       | 2/8 [00:00<00:00, 17.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 \t avg_loss 36.5849 \t vld_loss 8.3653 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00,  8.60it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 36.09it/s]\n",
      " 25%|██▌       | 2/8 [00:00<00:00, 13.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 \t avg_loss 24.5055 \t vld_loss 8.7215 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "SPLIT 3 --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00,  9.69it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 31.97it/s]\n",
      " 38%|███▊      | 3/8 [00:00<00:00, 18.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 \t avg_loss 32.5792 \t vld_loss 21.0915 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00,  8.26it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 31.19it/s]\n",
      " 25%|██▌       | 2/8 [00:00<00:00, 12.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 \t avg_loss 16.4998 \t vld_loss 21.6967 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "SPLIT 4 --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 10.25it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 22.39it/s]\n",
      " 25%|██▌       | 2/8 [00:00<00:00, 16.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 \t avg_loss 52.3153 \t vld_loss 13.9593 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 10.65it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 20.41it/s]\n",
      " 12%|█▎        | 1/8 [00:00<00:00,  8.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 \t avg_loss 22.2382 \t vld_loss 15.3426 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "SPLIT 5 --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 12.90it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.40it/s]\n",
      " 25%|██▌       | 2/8 [00:00<00:00, 12.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 \t avg_loss 16.9142 \t vld_loss 55.5484 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 13.18it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 \t avg_loss 9.2611 \t vld_loss 70.0871 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for sid in range(split_num):\n",
    "    trn_ixs, vld_ixs = train_test_split(\n",
    "        list(range(len(trn_X))), test_size=1 / split_num,\n",
    "        shuffle=True, random_state=sid)\n",
    "    print('SPLIT {} --------------------'.format(sid + 1))\n",
    "    model = BiLSTMCRF(len(word_to_ix), embed_dim=64,\n",
    "                      hid_dim=64, device=device).to(device)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=0.01, weight_decay=1e-6)\n",
    "    for epoch in range(epoch_num):\n",
    "        model.train()\n",
    "        model, avg_loss = train(trn_ixs, model, optimizer)\n",
    "        model.eval()\n",
    "        vld_loss, y_preds, y_trues = evaluate(vld_ixs, model)\n",
    "        print('Epoch {}/{} \\t avg_loss {:.4f} \\t vld_loss {:.4f} \\t'.format(\n",
    "            epoch + 1, epoch_num, avg_loss, vld_loss))\n",
    "        evalinfo(y_preds, y_trues)\n",
    "        savemodel(model, 'bilstmcrf_{}_{}.pytorch'.format(sid,epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder + Att + Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPLIT 1 --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  5.63it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  9.37it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 \t avg_loss -6.0156 \t vld_loss -47.1588 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  5.03it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  9.63it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 \t avg_loss -188.2837 \t vld_loss -432.4019 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "SPLIT 2 --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  5.40it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 26.26it/s]\n",
      " 12%|█▎        | 1/8 [00:00<00:00,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 \t avg_loss -34.2251 \t vld_loss -150.3462 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.76it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 25.90it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 \t avg_loss -359.8362 \t vld_loss -722.7305 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "SPLIT 3 --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  5.04it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 20.71it/s]\n",
      " 12%|█▎        | 1/8 [00:00<00:01,  5.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 \t avg_loss -10.7636 \t vld_loss -58.5708 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  5.14it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 22.91it/s]\n",
      " 12%|█▎        | 1/8 [00:00<00:01,  6.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 \t avg_loss -232.9170 \t vld_loss -470.5330 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "SPLIT 4 --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  5.36it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 16.21it/s]\n",
      " 12%|█▎        | 1/8 [00:00<00:01,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 \t avg_loss -86.6671 \t vld_loss -249.4149 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  5.15it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 16.28it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 \t avg_loss -546.4884 \t vld_loss -883.2411 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "SPLIT 5 --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  7.01it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.10it/s]\n",
      " 12%|█▎        | 1/8 [00:00<00:01,  5.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 \t avg_loss -73.4287 \t vld_loss -251.7889 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  5.89it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 \t avg_loss -509.2907 \t vld_loss -908.3806 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for sid in range(split_num):\n",
    "    trn_ixs, vld_ixs = train_test_split(\n",
    "        list(range(len(trn_X))), test_size=1 / split_num,\n",
    "        shuffle=True, random_state=sid)\n",
    "    print('SPLIT {} --------------------'.format(sid + 1))\n",
    "\n",
    "    model = EncoderAttDecoder(len(word_to_ix), in_hdim=en_indim, out_hdim=en_outdim,\n",
    "                              de_hdim=en_outdim * direction, max_length=max_length,\n",
    "                              bi=direction, dropout=0.3, num_layers=2,\n",
    "                              teacher_forcing_ratio=0.3, use_crf=False, device=device).to(device)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=0.01, weight_decay=1e-6)\n",
    "    for epoch in range(epoch_num):\n",
    "        model.train()\n",
    "        model, avg_loss = train(trn_ixs, model, optimizer)\n",
    "        model.eval()\n",
    "        vld_loss, y_preds, y_trues = evaluate(vld_ixs, model)\n",
    "        print('Epoch {}/{} \\t avg_loss {:.4f} \\t vld_loss {:.4f} \\t'.format(\n",
    "            epoch + 1, epoch_num, avg_loss, vld_loss))\n",
    "        evalinfo(y_preds, y_trues)\n",
    "        savemodel(model, 'ead_{}_{}.pytorch'.format(sid,epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder + Att + Decoder + CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPLIT 1 --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  5.41it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.06it/s]\n",
      " 12%|█▎        | 1/8 [00:00<00:00,  9.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 \t avg_loss 35.1029 \t vld_loss 39.7248 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.87it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.48it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 \t avg_loss 23.0273 \t vld_loss 39.0893 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "SPLIT 2 --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:02<00:00,  3.96it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 15.75it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 \t avg_loss 104.6514 \t vld_loss 21.1126 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:02<00:00,  3.35it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 16.02it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 \t avg_loss 25.5018 \t vld_loss 25.2135 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "SPLIT 3 --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:02<00:00,  3.47it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 16.55it/s]\n",
      " 12%|█▎        | 1/8 [00:00<00:00,  8.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 \t avg_loss 43.2810 \t vld_loss 28.5988 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:02<00:00,  3.41it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 16.99it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 \t avg_loss 48.5163 \t vld_loss 21.3842 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "SPLIT 4 --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  3.94it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 11.61it/s]\n",
      " 12%|█▎        | 1/8 [00:00<00:00,  9.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 \t avg_loss 66.1944 \t vld_loss 17.9390 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  3.28it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  9.82it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 \t avg_loss 34.0217 \t vld_loss 15.4507 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "SPLIT 5 --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  5.63it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.95it/s]\n",
      " 25%|██▌       | 2/8 [00:00<00:00, 11.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 \t avg_loss 29.8988 \t vld_loss 45.1030 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  5.31it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 \t avg_loss 14.5222 \t vld_loss 64.8156 \t\n",
      "TAG a \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG b \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "TAG c \t prec 0.0000 \t recl 0.0000 \t f1 0.0000\n",
      "AVG prec 0.0000 \t recl 0.0000 \t f1 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for sid in range(split_num):\n",
    "    trn_ixs, vld_ixs = train_test_split(\n",
    "        list(range(len(trn_X))), test_size=1 / split_num,\n",
    "        shuffle=True, random_state=sid)\n",
    "    print('SPLIT {} --------------------'.format(sid + 1))\n",
    "\n",
    "    model = EncoderAttDecoder(len(word_to_ix), in_hdim=en_indim, out_hdim=en_outdim,\n",
    "                              de_hdim=en_outdim * direction, max_length=max_length,\n",
    "                              bi=direction, dropout=0.3, num_layers=2,\n",
    "                              teacher_forcing_ratio=0.3, use_crf=True, device=device).to(device)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=0.01, weight_decay=1e-6)\n",
    "    for epoch in range(epoch_num):\n",
    "        model.train()\n",
    "        model, avg_loss = train(trn_ixs, model, optimizer)\n",
    "        model.eval()\n",
    "        vld_loss, y_preds, y_trues = evaluate(vld_ixs, model)\n",
    "        print('Epoch {}/{} \\t avg_loss {:.4f} \\t vld_loss {:.4f} \\t'.format(\n",
    "            epoch + 1, epoch_num, avg_loss, vld_loss))\n",
    "        evalinfo(y_preds, y_trues)\n",
    "        savemodel(model, 'eadcrf_{}_{}.pytorch'.format(sid,epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
