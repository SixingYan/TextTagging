{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "0a909520ff1a2aa78a76bbae6534f58654a33924"
   },
   "outputs": [],
   "source": [
    "CUR_DIR = \"../input/\"\n",
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 170000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "max_len = 150 #80 # max number of words in a question to use\n",
    "batch_size = 1536\n",
    "train_epochs = 4\n",
    "SEED = 1029\n",
    "split_num = 3\n",
    "tqdm.pandas(desc='Progress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "ef1b84dd9bf62d1c0eab3ac2cc23144916a70723"
   },
   "outputs": [],
   "source": [
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    "          '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…',\n",
    "          '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─',\n",
    "          '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞',\n",
    "          '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }\n",
    "specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}\n",
    "mispell_dict_v1 = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n",
    "mispell_dict_v2 = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n",
    "mispell_dict = {**mispell_dict_v1, **mispell_dict_v2} # merge two dicts\n",
    "\n",
    "#stopword_dict = dict.fromkeys(list(stopwords.words('english')), ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "f4a855eba874a4bdee89061023f54995ea56898b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "df703cef705266a76b8cad31d50ffcafc54ee846"
   },
   "outputs": [],
   "source": [
    "\n",
    "def replace_punct(x):\n",
    "    x = str(x)\n",
    "    for p in punct_mapping:\n",
    "        x = x.replace(p, punct_mapping[p])\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for s in specials:\n",
    "        x = x.replace(s, specials[s])\n",
    "    return x\n",
    "\n",
    "def replace_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)\n",
    "    \n",
    "\n",
    "def load_and_prepc():\n",
    "    # load\n",
    "    train_df = pd.read_csv(CUR_DIR+\"train.csv\")#[0:5000]\n",
    "    test_df = pd.read_csv(CUR_DIR+\"test.csv\")#[0:5000]\n",
    "    print(\"Train shape : \", train_df.shape)\n",
    "    print(\"Test shape : \", test_df.shape)\n",
    "    \n",
    "    \n",
    "    # lower\n",
    "    train_df[\"question_text\"] = train_df[\n",
    "        \"question_text\"].progress_apply(lambda x: x.lower())\n",
    "    test_df[\"question_text\"] = test_df[\n",
    "        \"question_text\"].progress_apply(lambda x: x.lower())\n",
    "    print('complete lower')\n",
    "    \n",
    "    # Clean the text\n",
    "    train_df[\"question_text\"] = train_df[\n",
    "        \"question_text\"].progress_apply(lambda x: replace_punct(x))\n",
    "    test_df[\"question_text\"] = test_df[\n",
    "        \"question_text\"].progress_apply(lambda x: replace_punct(x))\n",
    "    print('complete punct')\n",
    "    \n",
    "    # Clean numbers\n",
    "    train_df[\"question_text\"] = train_df[\n",
    "        \"question_text\"].progress_apply(lambda x: replace_numbers(x))\n",
    "    test_df[\"question_text\"] = test_df[\n",
    "        \"question_text\"].progress_apply(lambda x: replace_numbers(x))\n",
    "    print('complete numbers')\n",
    "    \n",
    "    # Clean speelings\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(\n",
    "        lambda x: replace_typical_misspell(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(\n",
    "        lambda x: replace_typical_misspell(x))\n",
    "    print('complete speelings')\n",
    "    \n",
    "    ## fill up the missing values\n",
    "    train_X = train_df[\"question_text\"].fillna(\"_##_\").values\n",
    "    test_X = test_df[\"question_text\"].fillna(\"_##_\").values\n",
    "    \n",
    "    ## Tokenize the sentences\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(train_X)+list(test_X))\n",
    "    train_X_token = tokenizer.texts_to_sequences(train_X)\n",
    "    test_X_token = tokenizer.texts_to_sequences(test_X)\n",
    "    print('complete Tokenize')\n",
    "    \n",
    "    ## Pad the sentences \n",
    "    train_X_token = pad_sequences(train_X_token, maxlen=max_len)\n",
    "    test_X_token = pad_sequences(test_X_token, maxlen=max_len)\n",
    "\n",
    "    ## Get the target values\n",
    "    train_y = train_df['target'].values\n",
    "    \n",
    "    #shuffling the data\n",
    "    np.random.seed(SEED)\n",
    "    trn_idx = np.random.permutation(len(train_X))\n",
    "    train_X_token = train_X_token[trn_idx]\n",
    "    train_y = train_y[trn_idx]\n",
    "    \n",
    "    return train_X_token, test_X_token, train_y, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "e27901693bc0c5b169b42de387df023d8488486c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   4%|▎         | 47629/1306122 [00:00<00:02, 476285.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n",
      "Test shape :  (56370, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1306122/1306122 [00:01<00:00, 754718.58it/s]\n",
      "Progress: 100%|██████████| 56370/56370 [00:00<00:00, 695755.18it/s]\n",
      "Progress:   0%|          | 1686/1306122 [00:00<01:17, 16853.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete lower\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1306122/1306122 [00:53<00:00, 24514.38it/s]\n",
      "Progress: 100%|██████████| 56370/56370 [00:02<00:00, 25667.25it/s]\n",
      "Progress:   0%|          | 6462/1306122 [00:00<00:20, 64616.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete punct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1306122/1306122 [00:15<00:00, 82424.90it/s]\n",
      "Progress: 100%|██████████| 56370/56370 [00:00<00:00, 83067.02it/s]\n",
      "Progress:   0%|          | 2627/1306122 [00:00<00:49, 26269.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete numbers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1306122/1306122 [00:34<00:00, 37319.11it/s]\n",
      "Progress: 100%|██████████| 56370/56370 [00:01<00:00, 36789.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete speelings\n",
      "complete Tokenize\n",
      "Took 2.99 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_X, test_X, train_y, word_index = load_and_prepc()\n",
    "total_time = (time.time() - start_time) / 60\n",
    "print(\"Took {:.2f} minutes\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "1097d703f036d04ddfc887d7444eddbf4abba7de"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "966ac842ac335ea0a6bd1cb83a0ab748f7161ac0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=1029, replacement=False, sampling_strategy='auto')\n",
    "train_X, train_y = rus.fit_sample(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-input": false,
    "_uuid": "7b54613398ad2f83ff5d6dfbede2e5a623ce9e54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 5.33 minutes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_glove(word_index):\n",
    "    #return np.random.normal(-0.005838499, 0.48782197, (120000, 300))\n",
    "    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "    #all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = -0.005838499, 0.48782197 #all_embs.mean(), all_embs.std()\n",
    "    #embed_size = 300 #all_embs.shape[1]\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i-1] = embedding_vector # tokenizer index start from 1 (maintain 0)\n",
    "    return embedding_matrix \n",
    "\n",
    "def load_para(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    "    #all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = -0.0053247833, 0.49346462 #all_embs.mean(), all_embs.std()\n",
    "    #embed_size = all_embs.shape[1]\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i-1] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "start_time = time.time()\n",
    "embedding_matrix_1 = load_glove(word_index)\n",
    "embedding_matrix_2 = load_para(word_index)\n",
    "total_time = (time.time() - start_time) / 60\n",
    "print(\"Took {:.2f} minutes\".format(total_time))\n",
    "\n",
    "embedding_matrix = np.mean([embedding_matrix_1, embedding_matrix_2], axis=0)\n",
    "\n",
    "del embedding_matrix_1, embedding_matrix_2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3ed33738139cd0cd93960a635e7e885b2248e58c"
   },
   "source": [
    "# III. TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "c3cae939956a4df29ab8c2d74125f310dba540f6"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim\n",
    "        step_dim = self.step_dim\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)\n",
    "    \n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        hidden_size = 60\n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "        self.lstm = nn.GRU(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.gru = nn.GRU(hidden_size*2, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.lstm_attention = Attention(hidden_size*2, max_len)\n",
    "        self.gru_attention = Attention(hidden_size*2, max_len)\n",
    "        self.linear = nn.Linear(480, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))\n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        h_gru, _ = self.gru(h_lstm)\n",
    "        h_lstm_atten = self.lstm_attention(h_lstm)\n",
    "        h_gru_atten = self.gru_attention(h_gru)\n",
    "        avg_pool = torch.mean(h_gru, 1)\n",
    "        max_pool, _ = torch.max(h_gru, 1)\n",
    "        conc = torch.cat((h_lstm_atten, h_gru_atten, avg_pool, max_pool), 1)\n",
    "        conc = self.relu(self.linear(conc))\n",
    "        conc = self.dropout(conc)\n",
    "        out = self.out(conc)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "3be9003eef7a9b811d21e38c417d61e0e6be058b"
   },
   "outputs": [],
   "source": [
    "splits = list(StratifiedKFold(n_splits=split_num, shuffle=True, random_state=SEED).split(train_X, train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "411bd82d679cb5732caa3693a9bd7377615c0e8c"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def seed_torch(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "def train_nn(train_X, train_y, train_idx, valid_idx, test_loader):\n",
    "    x_train_fold = torch.tensor(train_X[train_idx], dtype=torch.long).cuda()\n",
    "    y_train_fold = torch.tensor(\n",
    "        train_y[train_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "    x_val_fold = torch.tensor(train_X[valid_idx], dtype=torch.long).cuda()\n",
    "    y_val_fold = torch.tensor(\n",
    "        train_y[valid_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "\n",
    "    model = NeuralNet()\n",
    "    model.cuda()\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for epoch in range(train_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
    "        test_preds_fold = np.zeros(len(test_X))\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "            y_pred = model(x_batch).detach()\n",
    "            valid_preds_fold[\n",
    "                i * batch_size:(i + 1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "    for i, (x_batch,) in enumerate(test_loader):\n",
    "        y_pred = model(x_batch).detach()\n",
    "        test_preds_fold[\n",
    "            i * batch_size:(i + 1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "    del model\n",
    "    gc.collect()\n",
    "    return valid_preds_fold, test_preds_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "2b60490a60a0ca2035d330c3f9533387c71b5c79"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "38f555bb4a64987e6dbca48873b8acbb08b43e77"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "2710a96c402370ecfb5ee08c70dd61a5c272d5ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/4 \t loss=887.1128 \t val_loss=707.8874 \t time=30.04s\n",
      "Epoch 2/4 \t loss=698.7373 \t val_loss=582.4616 \t time=30.16s\n",
      "Epoch 3/4 \t loss=627.5162 \t val_loss=556.9655 \t time=30.53s\n",
      "Epoch 4/4 \t loss=580.6192 \t val_loss=511.7536 \t time=30.74s\n",
      "Fold 2\n",
      "Epoch 1/4 \t loss=887.8930 \t val_loss=709.4986 \t time=30.95s\n",
      "Epoch 2/4 \t loss=684.3137 \t val_loss=608.1898 \t time=31.09s\n",
      "Epoch 3/4 \t loss=618.5210 \t val_loss=594.0424 \t time=31.01s\n",
      "Epoch 4/4 \t loss=571.6978 \t val_loss=537.3799 \t time=31.24s\n",
      "Fold 3\n",
      "Epoch 1/4 \t loss=892.8866 \t val_loss=685.6042 \t time=31.18s\n",
      "Epoch 2/4 \t loss=691.3345 \t val_loss=592.8940 \t time=31.00s\n",
      "Epoch 3/4 \t loss=623.6547 \t val_loss=539.6399 \t time=31.22s\n",
      "Epoch 4/4 \t loss=574.3595 \t val_loss=514.3590 \t time=31.04s\n"
     ]
    }
   ],
   "source": [
    "train_preds = np.zeros((len(train_X)))\n",
    "test_preds = np.zeros((len(test_X)))\n",
    "\n",
    "seed_torch(SEED)\n",
    "\n",
    "x_test_cuda = torch.tensor(test_X, dtype=torch.long).cuda()\n",
    "test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "    x_train_fold = torch.tensor(train_X[train_idx], dtype=torch.long).cuda()\n",
    "    y_train_fold = torch.tensor(train_y[train_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "    x_val_fold = torch.tensor(train_X[valid_idx], dtype=torch.long).cuda()\n",
    "    y_val_fold = torch.tensor(train_y[valid_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "    \n",
    "    model = NeuralNet()\n",
    "    model.cuda()\n",
    "    \n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f'Fold {i + 1}')\n",
    "    \n",
    "    for epoch in range(train_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
    "        test_preds_fold = np.zeros(len(test_X))\n",
    "        avg_val_loss = 0.\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "            y_pred = model(x_batch).detach()\n",
    "            avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "            valid_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "        \n",
    "        elapsed_time = time.time() - start_time \n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n",
    "            epoch + 1, train_epochs, avg_loss, avg_val_loss, elapsed_time))\n",
    "        \n",
    "    for i, (x_batch,) in enumerate(test_loader):\n",
    "        y_pred = model(x_batch).detach()\n",
    "\n",
    "        test_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "\n",
    "    train_preds[valid_idx] = valid_preds_fold\n",
    "    test_preds += test_preds_fold / len(splits)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "40177f000fb636b77748639324ef0109a0e799de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 39.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'threshold': 0.39, 'f1': 0.8574990514391024}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def threshold_search(y_true, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in tqdm([i * 0.01 for i in range(100)]):\n",
    "        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'f1': best_score}\n",
    "    return search_result\n",
    "search_result = threshold_search(train_y, train_preds)\n",
    "search_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f96b12fa573d20260b98e16f7bfb712d2aaec15f"
   },
   "source": [
    "# VI. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "aa600544f87ff345da87b1feaf9d767a0b45f191"
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../input/sample_submission.csv')\n",
    "sub.prediction = test_preds > search_result['threshold']\n",
    "#sub.prediction = test_preds_output == 1\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
